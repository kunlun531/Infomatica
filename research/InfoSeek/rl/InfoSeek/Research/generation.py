import torch
import re
import os
import time
import random
import math
import requests
import json
from collections import deque
from typing import List, Dict, Any, Tuple, Optional, Union
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed

from openai import OpenAI
from tqdm import tqdm

from .tensor_helper import TensorHelper, TensorConfig
from verl import DataProto

# --- VLLM Server Configuration ---
# Configuration for connecting to VLLM server replicas for parallel inference.
# It's recommended to configure these via environment variables.
VLLM_SERVER_IP = os.getenv("VLLM_SERVER_IP", "127.0.0.1")
VLLM_BASE_PORT = int(os.getenv("VLLM_BASE_PORT", 8001))
NUM_REPLICAS = int(os.getenv("VLLM_NUM_REPLICAS", 4))
VLLM_API_URLS = [f"http://{VLLM_SERVER_IP}:{VLLM_BASE_PORT + i}/synthesize" for i in range(NUM_REPLICAS)]

print("VLLM client is configured to use the following server replicas for parallel processing:")
for url in VLLM_API_URLS:
    print(f"- {url}")


def thread_worker(chunk_payload: dict, server_url: str) -> List[str]:
    """
    Worker function for a single thread to send a small chunk of data to a specified server.
    This function is self-contained to ensure thread safety.
    """
    try:
        response = requests.post(server_url, json=chunk_payload, timeout=600)
        response.raise_for_status()  # Raises an exception for 4xx or 5xx status codes
        data = response.json()
        return data.get("search_results_processed", [])
    except requests.exceptions.RequestException as e:
        print(f"Error calling {server_url}: {e}")
        num_items = len(chunk_payload.get("queries", []))
        return ['[ERROR: Request Failed]'] * num_items
    except json.JSONDecodeError:
        print(f"Error decoding JSON from {server_url}. Response: {response.text}")
        num_items = len(chunk_payload.get("queries", []))
        return ['[ERROR: JSON Decode Failed]'] * num_items


def get_synthesized_information(
    original_questions: List[str],
    search_queries: List[str],
    search_results: List[str]
) -> List[str]:
    """
    Efficiently synthesizes information by splitting a large batch of requests
    and sending them in parallel to all VLLM server replicas using multiple threads.

    Args:
        original_questions: List of original user questions.
        search_queries: List of search queries generated by the agent.
        search_results: List of retrieved documents corresponding to the search queries.

    Returns:
        A list of synthesized information strings.
    """
    print("*" * 86)
    print("*" * 35 + " Synthesis Start " + "*" * 35)
    total_items = len(search_queries)
    if total_items == 0:
        return []

    print(f"Received a large payload of {total_items} items. Starting parallel processing across {NUM_REPLICAS} servers...")
    start_time = time.time()

    # 1. Split the large payload into N smaller chunks (N <= NUM_REPLICAS).
    # This logic correctly handles cases where total_items < NUM_REPLICAS.
    chunk_size = math.ceil(total_items / NUM_REPLICAS)
    tasks = []
    for i in range(NUM_REPLICAS):
        start_index = i * chunk_size
        end_index = start_index + chunk_size
        if start_index >= total_items:
            break

        chunk_payload = {
            "original_questions": original_questions[start_index:end_index],
            "queries": search_queries[start_index:end_index],
            "documents": search_results[start_index:end_index]
        }
        tasks.append({
            "index": i,
            "payload": chunk_payload,
            "url": VLLM_API_URLS[i]
        })

    # 2. Use a thread pool to send all requests in parallel.
    results_in_chunks = [[] for _ in range(len(tasks))]

    with ThreadPoolExecutor(max_workers=NUM_REPLICAS) as executor:
        future_to_task_index = {
            executor.submit(thread_worker, task["payload"], task["url"]): task["index"]
            for task in tasks
        }

        print(f"Dispatched {len(tasks)} chunks to servers. Waiting for responses...")

        for future in as_completed(future_to_task_index):
            task_index = future_to_task_index[future]
            try:
                result_chunk = future.result()
                results_in_chunks[task_index] = result_chunk
                print(f"Chunk {task_index} (sent to {tasks[task_index]['url']}) completed successfully.")
                print(f"Processed result example: {result_chunk[0].strip()[:100]} ...")
                print("-" * 60)
            except Exception as exc:
                print(f"Chunk {task_index} generated an exception: {exc}")
                num_failed_items = len(tasks[task_index]["payload"]["queries"])
                results_in_chunks[task_index] = ['[ERROR: Future Exception]'] * num_failed_items

    # 3. Concatenate all results in their original order.
    final_synthesized_results = []
    for chunk in results_in_chunks:
        final_synthesized_results.extend(chunk)

    end_time = time.time()
    if len(final_synthesized_results) != total_items:
        print(f"Warning: Final result count ({len(final_synthesized_results)}) does not match input count ({total_items}). Check for errors above.")
    print(f"\nAll chunks processed. Total time for {total_items} items: {end_time - start_time:.2f} seconds.")
    return final_synthesized_results


def extract_info(processed_list: List[str], original_list: List[str]) -> List[str]:
    """
    Extracts content from within <information> tags from a list of processed texts.
    If extraction fails or the tag is not found, it falls back to a truncated version of the original text.
    """
    final_results = []
    pattern = re.compile(r'<information>(.*?)</information>', re.DOTALL)

    for i, original_item in enumerate(original_list):
        try:
            processed_text = processed_list[i]
            match = pattern.search(processed_text)
            
            if match:
                extracted_content = match.group(1).strip()
                if extracted_content:
                    final_results.append(extracted_content)
                else: # Fallback if content inside tag is empty
                    final_results.append(original_item[:2048])
            else: # Fallback if tag is not found
                final_results.append(original_item[:2048])
        except (IndexError, TypeError): # Fallback for malformed input
            final_results.append(original_item[:2048])
    return final_results


@dataclass
class GenerationConfig:
    """Configuration for the LLM generation process."""
    max_turns: int
    max_start_length: int
    max_prompt_length: int
    max_response_length: int
    max_obs_length: int
    num_gpus: int
    no_think_rl: bool = False
    search_url: str = None
    topk: int = 3
    dense_retrieval_enable: bool = True
    elastic_search_enable: bool = False
    elastic_search_topk: int = 3


class LLMGenerationManager:
    """Manages the entire LLM generation loop, including interaction with the environment."""
    def __init__(
        self,
        tokenizer,
        actor_rollout_wg,
        config: GenerationConfig,
        is_validation: bool = False,
    ):
        self.tokenizer = tokenizer
        self.actor_rollout_wg = actor_rollout_wg
        self.config = config
        self.is_validation = is_validation
        self.info_token_ids = self.tokenizer("</information>", add_special_tokens=False)['input_ids']
        print(f"Pre-encoded </information> token IDs: {self.info_token_ids}")
        print(f"Dense retrieval enable: {self.config.dense_retrieval_enable}")
        print(f"Elastic search enable: {self.config.elastic_search_enable}")

        self.tensor_fn = TensorHelper(TensorConfig(
            pad_token_id=tokenizer.pad_token_id,
            max_prompt_length=config.max_prompt_length,
            max_obs_length=config.max_obs_length,
            max_start_length=config.max_start_length
        ))


    def _batch_tokenize(self, responses: List[str]) -> torch.Tensor:
        """Tokenize a batch of responses."""
        return self.tokenizer(
            responses,
            add_special_tokens=False,
            return_tensors='pt',
            padding="longest"
        )['input_ids']

    def _postprocess_responses(self, responses: torch.Tensor) -> torch.Tensor:
        """Process responses to stop at search operation or answer operation."""
        responses_str = self.tokenizer.batch_decode(
            responses,
            skip_special_tokens=True
        )

        responses_str = [resp.split('</search>')[0] + '</search>'
                         if '</search>' in resp
                         else resp.split('</answer>')[0] + '</answer>'
                         if '</answer>' in resp
                         else resp
                         for resp in responses_str]

        if self.config.no_think_rl:
            raise NotImplementedError("no_think_rl mode is not fully implemented.")

        responses = self._batch_tokenize(responses_str)
        return responses, responses_str

    def _process_next_obs(self, next_obs: List[str]) -> torch.Tensor:
        """
        Processes the next batch of observations from the environment.
        If an observation is too long, it is truncated, and an '</information>' tag
        is appended to the sequences that were actually truncated.
        """
        start_time = time.time()
        info_tokens = torch.tensor(self.info_token_ids, dtype=torch.long)
        info_len = len(self.info_token_ids)

        # 1. Encode the entire batch at once to get IDs and attention mask.
        encoded = self.tokenizer(
            next_obs,
            padding='longest',
            truncation=False,  # Do not truncate yet to check original lengths
            return_tensors='pt',
            add_special_tokens=False,
        )
        next_obs_ids = encoded['input_ids']
        attention_mask = encoded['attention_mask']

        # 2. If no truncation is needed, return early.
        if next_obs_ids.shape[1] <= self.config.max_obs_length:
            duration = time.time() - start_time
            print("*" * 86)
            print("*" * 35 + " Rollout Start " + "*" * 35)
            print(f"Process observation execution time: {duration:.4f} seconds (no truncation needed)")
            return next_obs_ids

        print(f"Warning: Observation too long, truncating from {next_obs_ids.shape[1]} to {self.config.max_obs_length}")

        # 3. Find which sequences exceed the max length.
        original_lengths = attention_mask.sum(dim=1)
        to_modify_mask = original_lengths > self.config.max_obs_length

        # 4. Truncate the entire batch to the target length.
        processed_ids = next_obs_ids[:, :self.config.max_obs_length]

        # 5. For the truncated sequences, insert the '</information>' token IDs at the end.
        info_tokens = info_tokens.to(processed_ids.device)
        replace_start_idx = self.config.max_obs_length - info_len
        processed_ids[to_modify_mask, replace_start_idx:] = info_tokens

        duration = time.time() - start_time
        print(f"Process observation execution time: {duration:.4f} seconds (truncation applied)")
        return processed_ids

    def _update_rolling_state(self, rollings: DataProto, cur_responses: torch.Tensor,
                              next_obs_ids: torch.Tensor) -> Dict:
        """Update rolling state with new responses and observations."""
        new_input_ids = self.tensor_fn.concatenate_with_padding([
            rollings.batch['input_ids'],
            cur_responses,
            next_obs_ids
        ])

        new_attention_mask = self.tensor_fn.create_attention_mask(new_input_ids)
        new_position_ids = self.tensor_fn.create_position_ids(new_attention_mask)

        effective_len = new_attention_mask.sum(dim=1).max()
        max_len = min(self.config.max_prompt_length, effective_len)

        new_rollings = DataProto.from_dict({
            'input_ids': new_input_ids[:, -max_len:],
            'position_ids': new_position_ids[:, -max_len:],
            'attention_mask': new_attention_mask[:, -max_len:]
        })
        new_rollings.meta_info.update(rollings.meta_info)

        return new_rollings

    def _info_masked_concatenate_with_padding(self,
                                                prompt: torch.Tensor,
                                                prompt_with_mask: torch.Tensor,
                                                response: torch.Tensor,
                                                info: torch.Tensor = None,
                                                pad_to_left: bool = True
                                                ) -> torch.Tensor:
        """Concatenate tensors and handle padding. Additionally, create a mask to cover the information block."""
        pad_id = self.tokenizer.pad_token_id
        tensors = [prompt, response]
        tensors_with_mask = [prompt_with_mask, response]
        if info is not None:
            tensors.append(info)
            info_mask = torch.full(info.size(), pad_id, dtype=info.dtype, device=info.device)
            tensors_with_mask.append(info_mask)

        concatenated = torch.cat(tensors, dim=1)
        concatenated_with_info = torch.cat(tensors_with_mask, dim=1)
        mask = concatenated != pad_id if pad_to_left else concatenated == pad_id
        sorted_indices = mask.to(torch.int64).argsort(dim=1, stable=True)
        padded_tensor = concatenated.gather(1, sorted_indices)
        padded_tensor_with_info = concatenated_with_info.gather(1, sorted_indices)

        return padded_tensor, padded_tensor_with_info

    def _update_right_side(self, right_side: Dict,
                           cur_responses: torch.Tensor,
                           next_obs_ids: torch.Tensor = None) -> Dict:
        """Update right side state, which tracks the generated responses and observations."""
        if next_obs_ids is not None:
            responses, responses_with_info_mask = self._info_masked_concatenate_with_padding(
                right_side['responses'],
                right_side['responses_with_info_mask'],
                cur_responses,
                next_obs_ids,
                pad_to_left=False
            )
        else:
            responses, responses_with_info_mask = self._info_masked_concatenate_with_padding(
                right_side['responses'],
                right_side['responses_with_info_mask'],
                cur_responses,
                pad_to_left=False
            )
        effective_len = self.tensor_fn.create_attention_mask(responses).sum(dim=1).max()
        max_len = min(self.config.max_prompt_length, effective_len)

        return {'responses': responses[:, :max_len], 'responses_with_info_mask': responses_with_info_mask[:, :max_len]}

    def _generate_with_gpu_padding(self, active_batch: DataProto) -> DataProto:
        """
        Wrapper for generation that handles multi-GPU padding requirements.
        If the active batch size is not divisible by num_gpus, it pads the batch
        with copies of the first sequence and then removes the padding from the output.
        """
        num_gpus = self.config.num_gpus
        if num_gpus <= 1:
            return self.actor_rollout_wg.generate_sequences(active_batch)

        batch_size = active_batch.batch['input_ids'].shape[0]
        remainder = batch_size % num_gpus

        for key in active_batch.batch.keys():
            active_batch.batch[key] = active_batch.batch[key].long()
        if remainder == 0:
            return self.actor_rollout_wg.generate_sequences(active_batch)

        # Add padding sequences
        padding_size = num_gpus - remainder
        padded_batch = {}

        for k, v in active_batch.batch.items():
            # Use the first sequence as a padding template
            pad_sequence = v[0:1].repeat(padding_size, *[1] * (len(v.shape) - 1))
            padded_batch[k] = torch.cat([v, pad_sequence], dim=0)

        padded_active_batch = DataProto.from_dict(padded_batch)
        for key in padded_active_batch.batch.keys():
            padded_active_batch.batch[key] = padded_active_batch.batch[key].long()

        input_ids = padded_active_batch.batch['input_ids']
        print(f"Input length padded: batch_size={input_ids.shape[0]}, seq_len={input_ids.shape[1]}")
        start_time = time.time()

        padded_output = self.actor_rollout_wg.generate_sequences(padded_active_batch)

        end_time = time.time()
        output_ids = padded_output.batch.get('input_ids')
        if output_ids is not None:
            print(f"Output length: batch_size={output_ids.shape[0]}, seq_len={output_ids.shape[1]}")
        print(f"Rollout generation time with padding: {end_time - start_time:.2f} seconds")
        print("*" * 35 + " Rollout End " + "*" * 35)
        print("*" * 86 + "\n")

        # Remove padding from the output
        trimmed_batch = {k: v[:-padding_size] for k, v in padded_output.batch.items()}

        if hasattr(padded_output, 'meta_info') and padded_output.meta_info:
            trimmed_meta = {}
            for k, v in padded_output.meta_info.items():
                if isinstance(v, torch.Tensor):
                    trimmed_meta[k] = v[:-padding_size]
                else:
                    trimmed_meta[k] = v
            padded_output.meta_info = trimmed_meta

        padded_output.batch = trimmed_batch
        return padded_output

    def run_llm_loop(self, gen_batch, batch_meta_info, initial_input_ids: torch.Tensor, epoch=0) -> Tuple[Dict, Dict]:
        """Run the main LLM generation loop for a batch of episodes."""

        # --- Extract original questions from the initial batch ---
        def _extract_question_from_prompt(prompt_text: str) -> str:
            """Extracts the question from a full prompt string using regex."""
            match = re.search(r"Question:\s*(.*?)\s*<\|im_end\|>", prompt_text, re.DOTALL)
            return match.group(1).strip() if match else "Original question not found."

        full_prompts_decoded = self.tokenizer.batch_decode(
            gen_batch.batch['input_ids'],
            skip_special_tokens=False
        )
        batch_original_questions = [_extract_question_from_prompt(p) for p in full_prompts_decoded]

        original_left_side = {'input_ids': initial_input_ids[:, -self.config.max_start_length:]}
        original_right_side = {'responses': initial_input_ids[:, []], 'responses_with_info_mask': initial_input_ids[:, []]}

        active_mask = torch.ones(gen_batch.batch['input_ids'].shape[0], dtype=torch.bool)
        turns_stats = torch.ones(gen_batch.batch['input_ids'].shape[0], dtype=torch.int)
        valid_action_stats = torch.zeros(gen_batch.batch['input_ids'].shape[0], dtype=torch.int)
        valid_search_stats = torch.zeros(gen_batch.batch['input_ids'].shape[0], dtype=torch.int)
        active_num_list = [active_mask.sum().item()]
        rollings = gen_batch

        # --- Main generation loop ---
        for step in range(self.config.max_turns):
            if not active_mask.sum():
                break
            rollings.batch = self.tensor_fn.cut_to_effective_len(
                rollings.batch,
                keys=['input_ids', 'attention_mask', 'position_ids']
            )

            rollings_active = DataProto.from_dict({
                k: v[active_mask] for k, v in rollings.batch.items()
            })
            gen_output = self._generate_with_gpu_padding(rollings_active)

            responses_ids, responses_str = self._postprocess_responses(gen_output.batch['responses'])
            responses_ids, responses_str = self.tensor_fn._example_level_pad(responses_ids, responses_str, active_mask)

            # Execute predictions in the environment and get observations
            next_obs, dones, valid_action, is_search = self.execute_predictions(
                batch_original_questions,
                responses_str,
                self.tokenizer.pad_token,
                active_mask
            )

            curr_active_mask = torch.tensor([not done for done in dones], dtype=torch.bool)
            active_mask = active_mask * curr_active_mask
            active_num_list.append(active_mask.sum().item())
            turns_stats[curr_active_mask] += 1
            valid_action_stats += torch.tensor(valid_action, dtype=torch.int)
            valid_search_stats += torch.tensor(is_search, dtype=torch.int)

            next_obs_ids = self._process_next_obs(next_obs)

            rollings = self._update_rolling_state(rollings, responses_ids, next_obs_ids)
            original_right_side = self._update_right_side(original_right_side, responses_ids, next_obs_ids)

        # --- Final LLM rollout for episodes that are still active ---
        if active_mask.sum():
            rollings.batch = self.tensor_fn.cut_to_effective_len(
                rollings.batch,
                keys=['input_ids', 'attention_mask', 'position_ids']
            )
            rollings_active = DataProto.from_dict({
                k: v[active_mask] for k, v in rollings.batch.items()
            })
            gen_output = self._generate_with_gpu_padding(rollings_active)

            responses_ids, responses_str = self._postprocess_responses(gen_output.batch['responses'])
            responses_ids, responses_str = self.tensor_fn._example_level_pad(responses_ids, responses_str, active_mask)

            # Execute final predictions without performing a search
            _, dones, valid_action, is_search = self.execute_predictions(
                batch_original_questions,
                responses_str,
                self.tokenizer.pad_token,
                active_mask,
                do_search=False
            )

            curr_active_mask = torch.tensor([not done for done in dones], dtype=torch.bool)
            active_mask = active_mask * curr_active_mask
            active_num_list.append(active_mask.sum().item())
            valid_action_stats += torch.tensor(valid_action, dtype=torch.int)
            valid_search_stats += torch.tensor(is_search, dtype=torch.int)
            original_right_side = self._update_right_side(original_right_side, responses_ids)

        meta_info = gen_output.meta_info if 'gen_output' in locals() else {}
        meta_info['turns_stats'] = turns_stats.tolist()
        meta_info['active_mask'] = active_mask.tolist()
        meta_info['valid_action_stats'] = valid_action_stats.tolist()
        meta_info['valid_search_stats'] = valid_search_stats.tolist()

        print("ACTIVE_TRAJ_NUM:", active_num_list)

        return self._compose_final_output(original_left_side, original_right_side, meta_info)

    def _compose_final_output(self, left_side: Dict,
                              right_side: Dict,
                              meta_info: Dict) -> Tuple[Dict, Dict]:
        """Compose the final generation output for the training process."""
        final_output = right_side.copy()
        final_output['prompts'] = left_side['input_ids']

        final_output['input_ids'] = torch.cat([
            left_side['input_ids'],
            right_side['responses']
        ], dim=1)

        final_output['attention_mask'] = torch.cat([
            self.tensor_fn.create_attention_mask(left_side['input_ids']),
            self.tensor_fn.create_attention_mask(final_output['responses'])
        ], dim=1)
        final_output['info_mask'] = torch.cat([
            self.tensor_fn.create_attention_mask(left_side['input_ids']),
            self.tensor_fn.create_attention_mask(final_output['responses_with_info_mask'])
        ], dim=1)

        final_output['position_ids'] = self.tensor_fn.create_position_ids(
            final_output['attention_mask']
        )

        final_output = DataProto.from_dict(final_output)
        final_output.meta_info.update(meta_info)

        return final_output

    def execute_predictions(self, original_questions: List[str], predictions: List[str], pad_token: str, active_mask=None, do_search=True) -> List[str]:
        """
        Executes predictions in the environment, performing searches and synthesizing information as needed.
        This function acts as the 'step' function for the environment interaction.
        """
        # 1. Parse predictions to extract actions, content, and query counts.
        cur_actions, contents, queries_num = self.postprocess_predictions(predictions)

        # 2. Filter data for all 'search' actions that are currently active.
        search_actions_data = [
            (q, content, num)
            for q, action, content, num, active in zip(original_questions, cur_actions, contents, queries_num, active_mask)
            if active and action == 'search'
        ]

        structured_search_info = []
        if search_actions_data and do_search:
            original_question_per_search, search_contents, search_counts_raw = zip(*search_actions_data)

            # Flatten the list of queries for batch searching
            search_query_groups = [c if c else [''] for c in search_contents]
            all_search_queries = [query for group in search_query_groups for query in group]
            search_action_query_counts = [count if count > 0 else 1 for count in search_counts_raw]

            start_search = time.time()
            flat_search_results = self.batch_search(all_search_queries)
            search_time = time.time() - start_search

            # Align original questions with each individual search query for synthesis
            original_questions_for_each_query = [
                q for q, count in zip(original_question_per_search, search_action_query_counts) for _ in range(count)
            ]

            start_synth = time.time()
            flat_synthesized_results = get_synthesized_information(
                original_questions_for_each_query, all_search_queries, flat_search_results
            )
            flat_processed_results = extract_info(flat_synthesized_results, flat_search_results)
            synth_time = time.time() - start_synth

            print(f"Search: {search_time:.3f}s | Synthesis: {synth_time:.3f}s")
            print("*" * 35 + " Synthesis End " + "*" * 35)
            print("*" * 86 + "\n")

            # 3. Reorganize the flat search results back into a structured format corresponding to each 'search' action.
            INFO_SEPARATOR = "\n\n[SNIPPET]\n\n"
            flat_queries_iter = iter(all_search_queries)
            flat_results_iter = iter(flat_processed_results)

            for count in search_action_query_counts:
                queries_for_this_action = [next(flat_queries_iter) for _ in range(count)]
                results_for_this_action = [next(flat_results_iter) for _ in range(count)]

                labeled_results = [
                    f"[Current Query]: {query}\n\n{snippet}"
                    for query, snippet in zip(queries_for_this_action, results_for_this_action)
                ]
                structured_search_info.append(INFO_SEPARATOR.join(labeled_results))

        search_results_queue = deque(structured_search_info)

        # 4. Construct the final output for each environment in the batch based on the action type.
        next_obs, dones, valid_action, is_search = [], [], [], []
        for i, (action, active) in enumerate(zip(cur_actions, active_mask)):
            if not active:
                next_obs.append('')
                dones.append(1)
                valid_action.append(0)
                is_search.append(0)
                continue

            if action == 'answer':
                next_obs.append('')
                dones.append(1)
                valid_action.append(1)
                is_search.append(0)
            elif action == 'search':
                search_info = search_results_queue.popleft() if search_results_queue else ""
                obs_str = f'\n\n<information>\n{search_info.strip()}\n</information>\n\n'
                next_obs.append(obs_str)
                dones.append(0)
                valid_action.append(1)
                is_search.append(1)
            else:  # Invalid action
                obs_str = '\nMy previous action is invalid. If I want to search, I should put the query between <search> and </search>. If I want to give the final answer, I should put the answer between <answer> and </answer>. Let me try again.\n'
                next_obs.append(obs_str)
                dones.append(0)
                valid_action.append(0)
                is_search.append(0)

        assert not search_results_queue, "Mismatch: Not all search results were consumed."
        return next_obs, dones, valid_action, is_search

    def postprocess_predictions(self, predictions: List[Any]) -> Tuple[List[Optional[str]], List[Union[str, List[str]]], List[Optional[int]]]:
        """
        Parses raw model predictions to extract actions ('search' or 'answer') and their content.
        """
        actions, contents, queries_num = [], [], []
        pattern = re.compile(r'<(search|answer)>(.*?)</\1>', re.DOTALL)

        for prediction in predictions:
            action, query_count = None, None
            content = ''  # Default to empty string

            if isinstance(prediction, str):
                match = pattern.search(prediction)
                if match:
                    action = match.group(1)
                    block_content = match.group(2).strip()

                    if action == 'search':
                        queries = []
                        for line in block_content.split('\n'):
                            cleaned_line = line.strip()
                            if cleaned_line:  # Ignore empty lines
                                # Remove leading numbers and dots (e.g., "1. " or "1.")
                                query = re.sub(r'^\s*\d+\.\s*', '', cleaned_line)
                                queries.append(query)
                        content = queries
                        query_count = len(queries)
                    elif action == 'answer':
                        content = block_content
                # If no tag is matched, default values are used
            else:
                # Handle non-string predictions gracefully
                print(f"Warning: Invalid prediction type encountered and skipped: {type(prediction)}")

            actions.append(action)
            contents.append(content)
            queries_num.append(query_count)
        return actions, contents, queries_num

    def batch_search(self, queries: List[str] = None) -> List[str]:
        """
        Performs a batch search for a list of queries.

        Args:
            queries: A list of queries to send to the search engine.
        Returns:
            A list of search results, where each result is a concatenated string of passages.
        """
        results = self._batch_search(queries)['result']
        return [self._passages2string(result) for result in results]

    def _batch_search(self, queries: List[str]) -> Dict:
        payload = {"queries": queries, "topk": self.config.topk, "return_scores": True}
        results = requests.post(self.config.search_url, json=payload, timeout=600).json()
        return results

    def _passages2string(self, retrieval_result: List[Dict]) -> str:
        """Formats a list of retrieved documents into a single string."""
        # Shuffle the retrieved documents with a small probability to mitigate bias.
        if random.random() < 0.1:
            random.shuffle(retrieval_result)
            
        format_reference = ''
        for idx, doc_item in enumerate(retrieval_result):
            content = doc_item.get('document', {}).get('contents', '')
            parts = content.split("\n")
            title = parts[0]
            text = "\n".join(parts[1:])
            format_reference += f"Doc {idx+1}(Title: {title}) {text}\n"

        return format_reference

    def merge_results(self, result_1: Dict, result_2: Dict) -> Dict:
        """Merges results from two different search engines query by query."""
        if 'result' not in result_1 or 'result' not in result_2:
            raise ValueError("Both inputs must have a 'result' key.")

        res1 = result_1['result']
        res2 = result_2['result']

        if len(res1) != len(res2):
            raise ValueError("The two result lists must have the same number of queries.")

        merged_results = []
        for i in range(len(res1)):
            if not isinstance(res1[i], list) or not isinstance(res2[i], list):
                raise TypeError(f"Each query result must be a list. Got {type(res1[i])} and {type(res2[i])} at index {i}")
            merged_results.append(res1[i] + res2[i])

        return {'result': merged_results}